{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf824bf-6de5-44dd-93fe-c11240615725",
   "metadata": {},
   "source": [
    "# Логистическая регрессия + Линейный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23192f-29b6-463e-9975-46c637e1b486",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\frac{d\\sigma}{dx}(x0) = \\sigma(x0) * (1 - \\sigma(x0))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fedf6-4fe7-4173-b18a-4a24558cae03",
   "metadata": {},
   "source": [
    "$$\n",
    "X -> X*W + b -> \\sigma(X_1) -> X_2*W_2 + b_2 -> P -> \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - P_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5c31f28-06ca-404f-95ea-565fe4bfb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward(weights, X, y):\n",
    "    X1 = X @ weights['W1'] + weights['B1']\n",
    "    X2 = sigmoid(X1)\n",
    "    P = X2 @ weights['W2'] + weights['B2']\n",
    "    P = P.flatten()\n",
    "    loss = np.mean((P - y) ** 2)\n",
    "    return loss, {'X1': X1, 'X2': X2, 'P': P}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7aaed797-204f-447f-b940-bd67396d8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(forward_, X, y, weights):\n",
    "    P = forward_['P']\n",
    "    N = len(y)\n",
    "    dL_dP = (2 / N) * (P - y) # <dL_dP, ...>\n",
    "    assert dL_dP.shape == (N,)\n",
    "    dL_dP = dL_dP.reshape(-1, 1)\n",
    "    dP_dW2 = forward_['X2'].T # left\n",
    "    dP_dX2 = weights['W2'].T # right\n",
    "    dP_dB2 = np.ones((N, 1))\n",
    "    W2_grad = (dP_dW2 @ dL_dP)\n",
    "    B2_grad = np.sum(dL_dP)\n",
    "    dX2_dX1 = forward_['X2'] * (1 - forward_['X2'])\n",
    "    dX1_dW1 = X.T # left\n",
    "    dL_dX1 = ((dL_dP @ dP_dX2) * dX2_dX1)\n",
    "    W1_grad = dX1_dW1 @ dL_dX1\n",
    "    B1_grad = np.sum(dL_dX1)\n",
    "    return {'W2': W2_grad, 'B2': B2_grad, 'W1': W1_grad, 'B1': B1_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1b1449-f657-4028-a962-f4ac1740d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "california = fetch_california_housing()\n",
    "x_train, x_test, y_train, y_test = train_test_split(california.data, california.target, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75550d3-b6e5-4d7f-a522-453c781a3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1043965-ab18-476e-9378-43855cc98c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(weights, x_train: np.ndarray, y_train: np.ndarray, lr = 0.1):\n",
    "    for s in range(1001):\n",
    "        loss, forward_info = forward(weights, x_train, y_train)\n",
    "        backward_info = backward(forward_info, x_train, y_train, weights)\n",
    "        weights['W1'] -= lr * backward_info['W1']\n",
    "        weights['W2'] -= lr * backward_info['W2']\n",
    "        weights['B1'] -= lr * backward_info['B1']\n",
    "        weights['B2'] -= lr * backward_info['B2']\n",
    "        if s % 10 == 0:\n",
    "            print(f'Iter {s}: loss = {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0966381-5a23-438f-89ec-8f5d4c6eb23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(x_train.shape[0], 4)\n",
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3b52b9b-ae09-4e7f-9a4e-0afb776e5f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: loss = 4.494365\n",
      "Iter 10: loss = 1.431252\n",
      "Iter 20: loss = 1.175610\n",
      "Iter 30: loss = 1.014667\n",
      "Iter 40: loss = 0.906707\n",
      "Iter 50: loss = 0.833564\n",
      "Iter 60: loss = 0.781889\n",
      "Iter 70: loss = 0.743140\n",
      "Iter 80: loss = 0.712498\n",
      "Iter 90: loss = 0.687242\n",
      "Iter 100: loss = 0.665718\n",
      "Iter 110: loss = 0.646938\n",
      "Iter 120: loss = 0.630452\n",
      "Iter 130: loss = 0.615959\n",
      "Iter 140: loss = 0.603035\n",
      "Iter 150: loss = 0.591290\n",
      "Iter 160: loss = 0.580432\n",
      "Iter 170: loss = 0.570262\n",
      "Iter 180: loss = 0.560669\n",
      "Iter 190: loss = 0.551594\n",
      "Iter 200: loss = 0.543008\n",
      "Iter 210: loss = 0.534895\n",
      "Iter 220: loss = 0.527243\n",
      "Iter 230: loss = 0.520041\n",
      "Iter 240: loss = 0.513278\n",
      "Iter 250: loss = 0.506940\n",
      "Iter 260: loss = 0.501009\n",
      "Iter 270: loss = 0.495469\n",
      "Iter 280: loss = 0.490299\n",
      "Iter 290: loss = 0.485479\n",
      "Iter 300: loss = 0.480986\n",
      "Iter 310: loss = 0.476801\n",
      "Iter 320: loss = 0.472903\n",
      "Iter 330: loss = 0.469272\n",
      "Iter 340: loss = 0.465889\n",
      "Iter 350: loss = 0.462738\n",
      "Iter 360: loss = 0.459800\n",
      "Iter 370: loss = 0.457060\n",
      "Iter 380: loss = 0.454500\n",
      "Iter 390: loss = 0.452107\n",
      "Iter 400: loss = 0.449864\n",
      "Iter 410: loss = 0.447761\n",
      "Iter 420: loss = 0.445783\n",
      "Iter 430: loss = 0.443921\n",
      "Iter 440: loss = 0.442163\n",
      "Iter 450: loss = 0.440501\n",
      "Iter 460: loss = 0.438927\n",
      "Iter 470: loss = 0.437433\n",
      "Iter 480: loss = 0.436012\n",
      "Iter 490: loss = 0.434658\n",
      "Iter 500: loss = 0.433367\n",
      "Iter 510: loss = 0.432132\n",
      "Iter 520: loss = 0.430949\n",
      "Iter 530: loss = 0.429816\n",
      "Iter 540: loss = 0.428727\n",
      "Iter 550: loss = 0.427681\n",
      "Iter 560: loss = 0.426673\n",
      "Iter 570: loss = 0.425702\n",
      "Iter 580: loss = 0.424765\n",
      "Iter 590: loss = 0.423860\n",
      "Iter 600: loss = 0.422985\n",
      "Iter 610: loss = 0.422139\n",
      "Iter 620: loss = 0.421320\n",
      "Iter 630: loss = 0.420525\n",
      "Iter 640: loss = 0.419755\n",
      "Iter 650: loss = 0.419008\n",
      "Iter 660: loss = 0.418283\n",
      "Iter 670: loss = 0.417578\n",
      "Iter 680: loss = 0.416893\n",
      "Iter 690: loss = 0.416226\n",
      "Iter 700: loss = 0.415578\n",
      "Iter 710: loss = 0.414947\n",
      "Iter 720: loss = 0.414333\n",
      "Iter 730: loss = 0.413735\n",
      "Iter 740: loss = 0.413151\n",
      "Iter 750: loss = 0.412583\n",
      "Iter 760: loss = 0.412028\n",
      "Iter 770: loss = 0.411487\n",
      "Iter 780: loss = 0.410959\n",
      "Iter 790: loss = 0.410443\n",
      "Iter 800: loss = 0.409939\n",
      "Iter 810: loss = 0.409447\n",
      "Iter 820: loss = 0.408966\n",
      "Iter 830: loss = 0.408496\n",
      "Iter 840: loss = 0.408035\n",
      "Iter 850: loss = 0.407585\n",
      "Iter 860: loss = 0.407144\n",
      "Iter 870: loss = 0.406712\n",
      "Iter 880: loss = 0.406289\n",
      "Iter 890: loss = 0.405874\n",
      "Iter 900: loss = 0.405467\n",
      "Iter 910: loss = 0.405068\n",
      "Iter 920: loss = 0.404676\n",
      "Iter 930: loss = 0.404291\n",
      "Iter 940: loss = 0.403912\n",
      "Iter 950: loss = 0.403540\n",
      "Iter 960: loss = 0.403174\n",
      "Iter 970: loss = 0.402813\n",
      "Iter 980: loss = 0.402458\n",
      "Iter 990: loss = 0.402108\n",
      "Iter 1000: loss = 0.401763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "W1 = np.random.randn(x_train.shape[1], 4)\n",
    "B1 = np.random.randn(1)\n",
    "W2 = np.random.randn(4, 1)\n",
    "B2 = np.random.randn(1)\n",
    "weights = {'W1': W1, 'W2': W2, 'B1': B1, 'B2': B2}\n",
    "learn(weights, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a745fdc-0c82-423b-b3d8-1878ed0d0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights):\n",
    "    x1 = x @ weights['W1'] + weights['B1']\n",
    "    x2 = sigmoid(x1)\n",
    "    p = x2 @ weights['W2'] + weights['B2']\n",
    "    return p.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df0d023b-bd19-4f5f-8c8d-51b2943bc5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.4541651383248917\n",
      "RMSE: 0.6445131808245368\n",
      "MSE: 0.4153972402565621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error\n",
    "\n",
    "y_pred = predict(x_test, weights)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred))\n",
    "print('RMSE:', root_mean_squared_error(y_test, y_pred))\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envlab)",
   "language": "python",
   "name": "envlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
